{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiao/anaconda3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import Dropout\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from utils import BOS_TOKEN, EOS_TOKEN, PAD_TOKEN\n",
    "from utils import BOW_TOKEN, EOW_TOKEN\n",
    "from utils import get_loader\n",
    "from vocab import Vocab, save_vocab\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(path, max_tok_len=None, max_seq_len=None):\n",
    "    # Read raw text file\n",
    "    # and build vocabulary for both words and chars\n",
    "    text = []  #[[],[]]  最外面的一层括号代表整个文本，里边的括号代表每一行\n",
    "    charset = {BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, BOW_TOKEN, EOW_TOKEN}\n",
    "    print(f\"Loading corpus from {path}\")\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f):\n",
    "            tokens = line.rstrip().split(\" \")\n",
    "            if max_seq_len is not None and len(tokens) + 2 > max_seq_len:\n",
    "                tokens = line[:max_seq_len-2]\n",
    "            sent = [BOS_TOKEN]\n",
    "            for token in tokens:\n",
    "                if max_tok_len is not None and len(token) + 2 > max_tok_len:\n",
    "                    token = token[:max_tok_len-2]\n",
    "                sent.append(token)\n",
    "                for ch in token:\n",
    "                    charset.add(ch)\n",
    "            sent.append(EOS_TOKEN)\n",
    "            text.append(sent)\n",
    "\n",
    "    # Build word and character vocabulary\n",
    "    print(\"Building word-level vocabulary\")\n",
    "    vocab_w = Vocab.build(\n",
    "        text,\n",
    "        min_freq=2,\n",
    "        reserved_tokens=[PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]\n",
    "    )\n",
    "    print(\"Building char-level vocabulary\")\n",
    "    vocab_c = Vocab(tokens=list(charset))\n",
    "\n",
    "    # Construct corpus using word_voab and char_vocab\n",
    "    corpus_w = [vocab_w.convert_tokens_to_ids(sent) for sent in text]\n",
    "    corpus_c = []\n",
    "    bow = vocab_c[BOW_TOKEN] #char2id\n",
    "    eow = vocab_c[EOW_TOKEN]\n",
    "    for i, sent in enumerate(text): #每一行\n",
    "        sent_c = []\n",
    "        for token in sent: #每一行里的每一个词 char2id\n",
    "            if token == BOS_TOKEN or token == EOS_TOKEN:\n",
    "                token_c = [bow, vocab_c[token], eow]\n",
    "            else:\n",
    "                token_c = [bow] + vocab_c.convert_tokens_to_ids(token) + [eow]\n",
    "            sent_c.append(token_c)\n",
    "        assert len(sent_c) == len(corpus_w[i])\n",
    "        corpus_c.append(sent_c)\n",
    "\n",
    "    assert len(corpus_w) == len(corpus_c)\n",
    "    return corpus_w, corpus_c, vocab_w, vocab_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class BiLMDataset(Dataset):\n",
    "    def __init__(self, corpus_w, corpus_c, vocab_w, vocab_c):\n",
    "        super(BiLMDataset, self).__init__()\n",
    "        self.pad_w = vocab_w[PAD_TOKEN]\n",
    "        self.pad_c = vocab_c[PAD_TOKEN]\n",
    "\n",
    "        self.data = []\n",
    "        for sent_w, sent_c in tqdm(zip(corpus_w, corpus_c)):\n",
    "            self.data.append((sent_w, sent_c))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def collate_fn(self, examples): #对齐\n",
    "        # lengths: batch_size\n",
    "        seq_lens = torch.LongTensor([len(ex[0]) for ex in examples])\n",
    "\n",
    "        # inputs_w\n",
    "        inputs_w = [torch.tensor(ex[0]) for ex in examples]\n",
    "        inputs_w = pad_sequence(inputs_w, batch_first=True, padding_value=self.pad_w) #自动找到序列中最长的token？？？？\n",
    "\n",
    "        # inputs_c: batch_size * max_seq_len * max_tok_len\n",
    "        batch_size, max_seq_len = inputs_w.shape\n",
    "        max_tok_len = max([max([len(tok) for tok in ex[1]]) for ex in examples]) #max(max(每一行中每一个词的长度)) -> 文档中所有词最长的长度\n",
    "\n",
    "        inputs_c = torch.LongTensor(batch_size, max_seq_len, max_tok_len).fill_(self.pad_c)\n",
    "        for i, (sent_w, sent_c) in enumerate(examples): #代表每一行\n",
    "            for j, tok in enumerate(sent_c): #一行中的每一个词\n",
    "                inputs_c[i][j][:len(tok)] = torch.LongTensor(tok)\n",
    "\n",
    "        # fw_input_indexes, bw_input_indexes = [], []\n",
    "        targets_fw = torch.LongTensor(inputs_w.shape).fill_(self.pad_w)\n",
    "        targets_bw = torch.LongTensor(inputs_w.shape).fill_(self.pad_w)\n",
    "        for i, (sent_w, sent_c) in enumerate(examples): #构造前向和后向的tar\n",
    "            targets_fw[i][:len(sent_w)-1] = torch.LongTensor(sent_w[1:]) \n",
    "            targets_bw[i][1:len(sent_w)] = torch.LongTensor(sent_w[:len(sent_w)-1])\n",
    "\n",
    "        return inputs_w, inputs_c, seq_lens, targets_fw, targets_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTokenEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_c,\n",
    "        char_embedding_dim,\n",
    "        char_conv_filters,\n",
    "        num_highways,\n",
    "        output_dim,\n",
    "        pad=\"<pad>\"\n",
    "    ):\n",
    "        super(ConvTokenEmbedder, self).__init__()\n",
    "        self.vocab_c = vocab_c\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(\n",
    "            len(vocab_c),\n",
    "            char_embedding_dim,\n",
    "            padding_idx=vocab_c[pad]\n",
    "        )\n",
    "        self.char_embeddings.weight.data.uniform_(-0.25, 0.25)\n",
    "\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        for kernel_size, out_channels in char_conv_filters:\n",
    "            conv = torch.nn.Conv1d(\n",
    "                in_channels=char_embedding_dim,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                bias=True\n",
    "            )\n",
    "            self.convolutions.append(conv)\n",
    "\n",
    "        self.num_filters = sum(f[1] for f in char_conv_filters)\n",
    "        self.num_highways = num_highways\n",
    "        self.highways = Highway(self.num_filters, self.num_highways, activation=F.relu)\n",
    "\n",
    "        self.projection = nn.Linear(self.num_filters, output_dim, bias=True)\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, token_len = inputs.shape\n",
    "        inputs = inputs.view(batch_size * seq_len, -1)\n",
    "        char_embeds = self.char_embeddings(inputs)\n",
    "        char_embeds = char_embeds.transpose(1, 2)\n",
    "\n",
    "        conv_hiddens = []\n",
    "        for i in range(len(self.convolutions)):\n",
    "            conv_hidden = self.convolutions[i](char_embeds)\n",
    "            conv_hidden, _ = torch.max(conv_hidden, dim=-1)\n",
    "            conv_hidden = F.relu(conv_hidden)\n",
    "            conv_hiddens.append(conv_hidden)\n",
    "\n",
    "        token_embeds = torch.cat(conv_hiddens, dim=-1)\n",
    "        token_embeds = self.highways(token_embeds)\n",
    "        token_embeds = self.projection(token_embeds)\n",
    "        token_embeds = token_embeds.view(batch_size, seq_len, -1)\n",
    "\n",
    "        return token_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMoLstmEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        dropout_prob=0.0\n",
    "    ):\n",
    "        super(ELMoLstmEncoder, self).__init__()\n",
    "\n",
    "        # set projection_dim==input_dim for ELMo usage\n",
    "        self.projection_dim = input_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.forward_layers = nn.ModuleList()\n",
    "        self.backward_layers = nn.ModuleList()\n",
    "        self.forward_projections = nn.ModuleList()\n",
    "        self.backward_projections = nn.ModuleList()\n",
    "\n",
    "        lstm_input_dim = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            forward_layer = nn.LSTM(\n",
    "                lstm_input_dim,\n",
    "                hidden_dim,\n",
    "                num_layers=1,\n",
    "                batch_first=True\n",
    "            )\n",
    "            forward_projection = nn.Linear(hidden_dim, self.projection_dim, bias=True)\n",
    "\n",
    "            backward_layer = nn.LSTM(\n",
    "                lstm_input_dim,\n",
    "                hidden_dim,\n",
    "                num_layers=1,\n",
    "                batch_first=True\n",
    "            )\n",
    "            backward_projection = nn.Linear(hidden_dim, self.projection_dim, bias=True)\n",
    "\n",
    "            lstm_input_dim = self.projection_dim\n",
    "\n",
    "            self.forward_layers.append(forward_layer)\n",
    "            self.forward_projections.append(forward_projection)\n",
    "            self.backward_layers.append(backward_layer)\n",
    "            self.backward_projections.append(backward_projection)\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        batch_size, seq_len, input_dim = inputs.shape\n",
    "        rev_idx = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1)\n",
    "        for i in range(lengths.shape[0]):\n",
    "            rev_idx[i,:lengths[i]] = torch.arange(lengths[i]-1, -1, -1)\n",
    "        rev_idx = rev_idx.unsqueeze(2).expand_as(inputs)\n",
    "        rev_idx = rev_idx.to(inputs.device)\n",
    "        rev_inputs = inputs.gather(1, rev_idx)\n",
    "\n",
    "        forward_inputs, backward_inputs = inputs, rev_inputs\n",
    "        stacked_forward_states, stacked_backward_states = [], []\n",
    "\n",
    "        for layer_index in range(self.num_layers):\n",
    "            # Transfer `lengths` to CPU to be compatible with latest PyTorch versions.\n",
    "            packed_forward_inputs = pack_padded_sequence(\n",
    "                forward_inputs, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            packed_backward_inputs = pack_padded_sequence(\n",
    "                backward_inputs, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "            # forward\n",
    "            forward_layer = self.forward_layers[layer_index]\n",
    "            packed_forward, _ = forward_layer(packed_forward_inputs)\n",
    "            forward = pad_packed_sequence(packed_forward, batch_first=True)[0]\n",
    "            forward = self.forward_projections[layer_index](forward)\n",
    "            stacked_forward_states.append(forward)\n",
    "\n",
    "            # backward\n",
    "            backward_layer = self.backward_layers[layer_index]\n",
    "            packed_backward, _ = backward_layer(packed_backward_inputs)\n",
    "            backward = pad_packed_sequence(packed_backward, batch_first=True)[0]\n",
    "            backward = self.backward_projections[layer_index](backward)\n",
    "            # convert back to original sequence order using rev_idx\n",
    "            stacked_backward_states.append(backward.gather(1, rev_idx))\n",
    "\n",
    "            forward_inputs, backward_inputs = forward, backward\n",
    "\n",
    "        # stacked_forward_states: [batch_size, seq_len, projection_dim] * num_layers\n",
    "        # stacked_backward_states: [batch_size, seq_len, projection_dim] * num_layers\n",
    "        return stacked_forward_states, stacked_backward_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLM(nn.Module):\n",
    "    \"\"\"\n",
    "    多层双向语言模型。\n",
    "    \"\"\"\n",
    "    def __init__(self, configs, vocab_w, vocab_c):\n",
    "        super(BiLM, self).__init__()\n",
    "        self.dropout_prob = configs['dropout_prob']\n",
    "        self.num_classes = len(vocab_w)\n",
    "\n",
    "        self.token_embedder = ConvTokenEmbedder(\n",
    "            vocab_c,\n",
    "            configs['char_embedding_dim'],\n",
    "            configs['char_conv_filters'],\n",
    "            configs['num_highways'],\n",
    "            configs['projection_dim']\n",
    "        )\n",
    "\n",
    "        self.encoder = ELMoLstmEncoder(\n",
    "            configs['projection_dim'],\n",
    "            configs['hidden_dim'],\n",
    "            configs['num_layers']\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(configs['projection_dim'], self.num_classes)\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        token_embeds = self.token_embedder(inputs)\n",
    "        token_embeds = F.dropout(token_embeds, self.dropout_prob)\n",
    "        forward, backward = self.encoder(token_embeds, lengths)\n",
    "\n",
    "        return self.classifier(forward[-1]), self.classifier(backward[-1])\n",
    "\n",
    "    def save_pretrained(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.token_embedder.state_dict(), os.path.join(path, 'token_embedder.pth'))\n",
    "        torch.save(self.encoder.state_dict(), os.path.join(path, 'encoder.pth'))\n",
    "        torch.save(self.classifier.state_dict(), os.path.join(path, 'classifier.pth'))\n",
    "\n",
    "    def load_pretrained(self, path):\n",
    "        self.token_embedder.load_state_dict(torch.load(os.path.join(path, 'token_embedder.pth')))\n",
    "        self.encoder.load_state_dict(torch.load(os.path.join(path, 'encoder.pth')))\n",
    "        self.classifier.load_state_dict(torch.load(os.path.join(path, 'classifier.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from ./train.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m configs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tok_len\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_file\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./train.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# path to your training file, line-by-line and tokenized\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 18\u001b[0m corpus_w, corpus_c, vocab_w, vocab_c \u001b[38;5;241m=\u001b[39m \u001b[43mload_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m train_data \u001b[38;5;241m=\u001b[39m BiLMDataset(corpus_w, corpus_c, vocab_w, vocab_c)\n\u001b[1;32m     20\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_loader(train_data, configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mload_corpus\u001b[0;34m(path, max_tok_len, max_seq_len)\u001b[0m\n\u001b[1;32m      5\u001b[0m charset \u001b[38;5;241m=\u001b[39m {BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, BOW_TOKEN, EOW_TOKEN}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading corpus from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(f):\n\u001b[1;32m      9\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/codecs.py:905\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    902\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# Force opening of the file in binary mode\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     mode \u001b[38;5;241m=\u001b[39m mode \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 905\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train.txt'"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    'max_tok_len': 50,\n",
    "    'train_file': './train.txt', # path to your training file, line-by-line and tokenized\n",
    "    'model_path': './elmo_bilm',\n",
    "    'char_embedding_dim': 50,\n",
    "    'char_conv_filters': [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]],\n",
    "    'num_highways': 2,\n",
    "    'projection_dim': 512,\n",
    "    'hidden_dim': 4096,\n",
    "    'num_layers': 2,\n",
    "    'batch_size': 32,\n",
    "    'dropout_prob': 0.1,\n",
    "    'learning_rate': 0.0004,\n",
    "    'clip_grad': 5,\n",
    "    'num_epoch': 10\n",
    "}\n",
    "\n",
    "corpus_w, corpus_c, vocab_w, vocab_c = load_corpus(configs['train_file'])\n",
    "train_data = BiLMDataset(corpus_w, corpus_c, vocab_w, vocab_c)\n",
    "train_loader = get_loader(train_data, configs['batch_size'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=vocab_w[PAD_TOKEN],\n",
    "    reduction=\"sum\"\n",
    ")\n",
    "print(\"Building BiLM model\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BiLM(configs, vocab_w, vocab_c)\n",
    "print(model)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda x: x.requires_grad, model.parameters()),\n",
    "    lr=configs['learning_rate']\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(configs['num_epoch']):\n",
    "    total_loss = 0\n",
    "    total_tags = 0 # number of valid predictions\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        batch = [x.to(device) for x in batch]\n",
    "        inputs_w, inputs_c, seq_lens, targets_fw, targets_bw = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs_fw, outputs_bw = model(inputs_c, seq_lens)\n",
    "        loss_fw = criterion(\n",
    "            outputs_fw.view(-1, outputs_fw.shape[-1]),\n",
    "            targets_fw.view(-1)\n",
    "        )\n",
    "        loss_bw = criterion(\n",
    "            outputs_bw.view(-1, outputs_bw.shape[-1]),\n",
    "            targets_bw.view(-1)\n",
    "        )\n",
    "        loss = (loss_fw + loss_bw) / 2.0\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), configs['clip_grad'])\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_fw.item()\n",
    "        total_tags += seq_lens.sum().item()\n",
    "\n",
    "    train_ppl = np.exp(total_loss / total_tags)\n",
    "    print(f\"Train PPL: {train_ppl:.2f}\")\n",
    "\n",
    "# save BiLM encoders\n",
    "model.save_pretrained(configs['model_path'])\n",
    "# save configs\n",
    "json.dump(configs, open(os.path.join(configs['model_path'], 'configs.json'), \"w\"))\n",
    "# save vocabularies\n",
    "save_vocab(vocab_w, os.path.join(configs['model_path'], 'word.dic'))\n",
    "save_vocab(vocab_c, os.path.join(configs['model_path'], 'char.dic'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "156f2ab0b5117c52916e123160272afa4393e9f725bf9ea3c56754300a0837a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
